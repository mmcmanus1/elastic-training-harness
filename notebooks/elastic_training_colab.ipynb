{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Elastic Fault-Tolerant Distributed Training Harness\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mmcmanus1/elastic-training-harness/blob/main/notebooks/elastic_training_colab.ipynb)\n\nThis notebook demonstrates the `elastic_harness` package for fault-tolerant distributed training:\n\n- **Multi-tier checkpointing** - Memory → NVMe → S3 fallback hierarchy\n- **LR scaling** - Automatic learning rate adjustment for topology changes\n- **Gradient accumulation** - Maintain constant global batch size\n- **Fault tolerance** - Resume from checkpoint after crashes\n- **Distributed training** - Using torchrun with elastic scaling"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Clone the repository and install the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and install the elastic-training-harness package\n",
    "!git clone https://github.com/mmcmanus1/elastic-training-harness.git\n",
    "%cd elastic-training-harness\n",
    "!pip install -e . --quiet\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import from elastic_harness Package\n",
    "\n",
    "Import the core components from the installed package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint management\n",
    "from elastic_harness.checkpoint import (\n",
    "    CheckpointConfig,\n",
    "    CheckpointState,\n",
    "    CheckpointManager,\n",
    "    CheckpointTier,\n",
    "    MemorySnapshotBackend,\n",
    "    NVMeBackend,\n",
    ")\n",
    "\n",
    "# LR scaling and gradient accumulation\n",
    "from elastic_harness.scaling import (\n",
    "    ScalingRule,\n",
    "    ScalingConfig,\n",
    "    LRScalingManager,\n",
    "    GradAccumulationConfig,\n",
    "    GradientAccumulationManager,\n",
    "    ElasticScalingManager,\n",
    ")\n",
    "\n",
    "# Distributed training utilities\n",
    "from elastic_harness.agent import (\n",
    "    setup_distributed_environment,\n",
    "    get_world_info,\n",
    ")\n",
    "\n",
    "print(\"Successfully imported elastic_harness components!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Model\n",
    "\n",
    "A simple transformer language model for demonstration. Sized for T4 GPU (16GB VRAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerLM(nn.Module):\n",
    "    \"\"\"Simple transformer language model for demonstration.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 50257,\n",
    "        d_model: int = 256,\n",
    "        nhead: int = 4,\n",
    "        num_layers: int = 2,\n",
    "        dim_feedforward: int = 512,\n",
    "        max_seq_length: int = 128,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_length, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "        self.output_proj.weight = self.embedding.weight  # Tie weights\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, labels=None):\n",
    "        seq_len = input_ids.size(1)\n",
    "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        x = self.embedding(input_ids) + self.pos_embedding(positions)\n",
    "\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(seq_len, device=input_ids.device)\n",
    "        x = self.transformer(x, mask=mask, is_causal=True)\n",
    "\n",
    "        logits = self.output_proj(x)\n",
    "\n",
    "        output = {\"logits\": logits}\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            output[\"loss\"] = loss\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Test model\n",
    "model = SimpleTransformerLM()\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training with Fault Tolerance\n",
    "\n",
    "Training loop using the `elastic_harness` checkpoint and scaling managers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_with_fault_tolerance(\n    model: nn.Module,\n    max_steps: int = 100,\n    batch_size: int = 4,\n    seq_length: int = 128,\n    vocab_size: int = 50257,\n    checkpoint_interval: int = 25,\n    memory_snapshot_interval: int = 5,\n    chaos_enabled: bool = False,\n    chaos_probability: float = 0.02,\n    chaos_after_step: int = 30,\n):\n    \"\"\"Training loop with fault tolerance using elastic_harness.\"\"\"\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n\n    # Setup optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n\n    # Setup LR scaling manager from elastic_harness\n    lr_config = ScalingConfig(\n        base_lr=1e-4,\n        base_batch_size=batch_size,\n        base_world_size=1,\n        scaling_rule=ScalingRule.LINEAR,\n        warmup_steps=10,\n    )\n    lr_manager = LRScalingManager(lr_config, optimizer)\n\n    # Setup checkpoint backends from elastic_harness\n    memory_backend = MemorySnapshotBackend(max_snapshots=2)\n    nvme_backend = NVMeBackend(base_path='/tmp/elastic_checkpoints')\n\n    # Try to resume from checkpoint\n    start_step = 0\n    latest_path = nvme_backend.get_latest_checkpoint()\n    if latest_path:\n        checkpoint = nvme_backend.load(latest_path)\n        model.load_state_dict(checkpoint.model_state_dict)\n        optimizer.load_state_dict(checkpoint.optimizer_state_dict)\n        start_step = checkpoint.step + 1\n        print(f\"Resumed from checkpoint at step {checkpoint.step}\")\n    else:\n        print(\"Starting fresh training run\")\n\n    # Synthetic data generator\n    def get_batch():\n        input_ids = torch.randint(0, vocab_size, (batch_size, seq_length), device=device)\n        labels = torch.randint(0, vocab_size, (batch_size, seq_length), device=device)\n        return input_ids, labels\n\n    # Training loop\n    model.train()\n    losses = []\n    step_times = []\n\n    print(f\"\\nStarting training from step {start_step} to {max_steps}\")\n    print(f\"Chaos mode: {'ENABLED' if chaos_enabled else 'disabled'}\")\n    print(\"-\" * 50)\n\n    for step in range(start_step, max_steps):\n        step_start = time.time()\n\n        # Chaos: maybe crash (for testing recovery)\n        if chaos_enabled and step > chaos_after_step:\n            if random.random() < chaos_probability:\n                print(f\"\\nCHAOS: Simulated crash at step {step}!\")\n                print(\"Run this cell again to resume from checkpoint.\")\n                return losses, step\n\n        # Memory snapshot (using elastic_harness backend)\n        if step > 0 and step % memory_snapshot_interval == 0:\n            state = CheckpointState(\n                step=step,\n                model_state_dict=model.state_dict(),\n                optimizer_state_dict=optimizer.state_dict(),\n            )\n            memory_backend.save(state)\n\n        # Get batch and forward pass\n        input_ids, labels = get_batch()\n        outputs = model(input_ids, labels=labels)\n        loss = outputs['loss']\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        # Step LR warmup\n        current_lr = lr_manager.step_warmup()\n\n        losses.append(loss.item())\n        step_times.append(time.time() - step_start)\n\n        # Logging\n        if step % 10 == 0:\n            avg_loss = sum(losses[-10:]) / min(10, len(losses))\n            avg_time = sum(step_times[-10:]) / min(10, len(step_times))\n            tokens_per_sec = batch_size * seq_length / avg_time\n            print(f\"Step {step:4d} | Loss: {avg_loss:.4f} | LR: {current_lr:.2e} | Tokens/s: {tokens_per_sec:.0f}\")\n\n        # NVMe checkpoint (using elastic_harness backend)\n        if step > 0 and step % checkpoint_interval == 0:\n            state = CheckpointState(\n                step=step,\n                model_state_dict=model.state_dict(),\n                optimizer_state_dict=optimizer.state_dict(),\n                metrics={'loss': loss.item()},\n            )\n            path = f\"checkpoint_step_{step:08d}.pt\"\n            nvme_backend.save(state, path)\n            nvme_backend.cleanup_old_checkpoints(keep_last=2)\n            print(f\"  [Checkpoint saved: {path}]\")\n            memory_backend.clear()  # Clear memory after persistent save\n\n    print(\"-\" * 50)\n    print(f\"Training complete! Final loss: {losses[-1]:.4f}\")\n    return losses, max_steps"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Training\n",
    "\n",
    "Run the training loop. If chaos mode is enabled and a crash occurs, re-run the cell to resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = SimpleTransformerLM(\n",
    "    vocab_size=50257,\n",
    "    d_model=256,\n",
    "    nhead=4,\n",
    "    num_layers=2,\n",
    "    dim_feedforward=512,\n",
    "    max_seq_length=128,\n",
    ")\n",
    "\n",
    "# Run training (set chaos_enabled=True to test recovery)\n",
    "losses, final_step = train_with_fault_tolerance(\n",
    "    model,\n",
    "    max_steps=100,\n",
    "    batch_size=4,\n",
    "    seq_length=128,\n",
    "    checkpoint_interval=25,\n",
    "    memory_snapshot_interval=5,\n",
    "    chaos_enabled=False,  # Set to True to test crash recovery\n",
    "    chaos_probability=0.05,\n",
    "    chaos_after_step=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Topology Change Simulation\n",
    "\n",
    "Demonstrate how `elastic_harness` handles dynamic world size changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Simulating Topology Changes\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Setup with elastic_harness scaling managers\n",
    "dummy_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "lr_config = ScalingConfig(\n",
    "    base_lr=1e-4,\n",
    "    base_batch_size=8,\n",
    "    base_world_size=4,\n",
    "    scaling_rule=ScalingRule.LINEAR,\n",
    ")\n",
    "lr_manager = LRScalingManager(lr_config, dummy_optimizer)\n",
    "\n",
    "accum_config = GradAccumulationConfig(\n",
    "    target_global_batch_size=256,\n",
    "    local_batch_size=8,\n",
    "    base_world_size=4,\n",
    ")\n",
    "accum_manager = GradientAccumulationManager(accum_config)\n",
    "\n",
    "print(f\"Initial state (4 GPUs):\")\n",
    "print(f\"  LR: {lr_manager.current_lr:.2e}\")\n",
    "print(f\"  Accumulation steps: {accum_manager.accumulation_steps}\")\n",
    "print(f\"  Global batch: {8 * 4 * accum_manager.accumulation_steps}\")\n",
    "print()\n",
    "\n",
    "# Simulate losing a GPU\n",
    "print(\"Worker failure: 4 GPUs -> 3 GPUs\")\n",
    "new_lr = lr_manager.on_topology_change(3)\n",
    "new_accum = accum_manager.on_topology_change(3)\n",
    "print(f\"  New LR: {new_lr:.2e}\")\n",
    "print(f\"  New accumulation steps: {new_accum}\")\n",
    "print(f\"  Effective global batch: {8 * 3 * new_accum}\")\n",
    "print()\n",
    "\n",
    "# Simulate adding GPUs\n",
    "print(\"Workers joining: 3 GPUs -> 6 GPUs\")\n",
    "new_lr = lr_manager.on_topology_change(6)\n",
    "new_accum = accum_manager.on_topology_change(6)\n",
    "print(f\"  New LR: {new_lr:.2e}\")\n",
    "print(f\"  New accumulation steps: {new_accum}\")\n",
    "print(f\"  Effective global batch: {8 * 6 * new_accum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Distributed Training Demo\n",
    "\n",
    "Run a multi-process training script that imports from `elastic_harness`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a distributed training script that uses elastic_harness\n",
    "training_script = '''\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '/content/elastic-training-harness/src')\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from elastic_harness.checkpoint import MemorySnapshotBackend, CheckpointState\n",
    "from elastic_harness.scaling import ScalingConfig, ScalingRule, LRScalingManager\n",
    "\n",
    "def main():\n",
    "    # Setup distributed\n",
    "    dist.init_process_group(backend=\"gloo\")\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "\n",
    "    print(f\"Worker {rank}/{world_size} started\")\n",
    "\n",
    "    # Simple model with DDP\n",
    "    model = nn.Linear(10, 10)\n",
    "    model = DDP(model)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Setup elastic_harness components\n",
    "    lr_config = ScalingConfig(\n",
    "        base_lr=0.01,\n",
    "        base_batch_size=4,\n",
    "        base_world_size=world_size,\n",
    "        scaling_rule=ScalingRule.LINEAR,\n",
    "    )\n",
    "    lr_manager = LRScalingManager(lr_config, optimizer)\n",
    "    memory_backend = MemorySnapshotBackend(max_snapshots=2)\n",
    "\n",
    "    # Training loop\n",
    "    for step in range(10):\n",
    "        x = torch.randn(4, 10)\n",
    "        loss = model(x).sum()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save memory snapshot every 5 steps\n",
    "        if step % 5 == 0 and step > 0:\n",
    "            state = CheckpointState(\n",
    "                step=step,\n",
    "                model_state_dict=model.module.state_dict(),\n",
    "                optimizer_state_dict=optimizer.state_dict(),\n",
    "            )\n",
    "            memory_backend.save(state)\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "    print(f\"Worker {rank} finished\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "with open('/tmp/distributed_demo.py', 'w') as f:\n",
    "    f.write(training_script)\n",
    "\n",
    "print(\"Distributed training script written to /tmp/distributed_demo.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run distributed training with 2 processes\n",
    "!torchrun --standalone --nproc-per-node=2 /tmp/distributed_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the `elastic_harness` package:\n",
    "\n",
    "1. **Package Installation** - Clone and install from GitHub\n",
    "2. **LR Scaling** - `LRScalingManager` adjusts learning rate on topology changes\n",
    "3. **Gradient Accumulation** - `GradientAccumulationManager` maintains constant global batch size\n",
    "4. **Multi-tier Checkpointing** - `MemorySnapshotBackend` and `NVMeBackend` with automatic cleanup\n",
    "5. **Fault Tolerance** - Resume from checkpoint after simulated crashes\n",
    "6. **Distributed Training** - Using torchrun with `elastic_harness` components\n",
    "\n",
    "For production use:\n",
    "- Use `CheckpointManager` for coordinated multi-tier checkpointing\n",
    "- Deploy etcd for multi-node rendezvous\n",
    "- Configure S3Backend for durable checkpoints\n",
    "- Use `ElasticScalingManager` for unified LR and accumulation management"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}