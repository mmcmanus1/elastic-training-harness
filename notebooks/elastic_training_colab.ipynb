{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Elastic Fault-Tolerant Distributed Training Harness\n",
        "\n",
        "This notebook demonstrates the elastic training harness that:\n",
        "- Automatically detects worker failures and re-balances workload\n",
        "- Resumes training from latest checkpoint with RTO < 30 seconds\n",
        "- Handles dynamic world_size changes without data duplication/skipping\n",
        "\n",
        "## Features\n",
        "- **Token-based data sharding** - Correct resumption even when topology changes\n",
        "- **Multi-tier checkpointing** - Memory → NVMe → S3 fallback hierarchy\n",
        "- **LR scaling** - Automatic learning rate adjustment for topology changes\n",
        "- **Gradient accumulation** - Maintain constant global batch size\n",
        "- **Chaos testing** - Built-in random crash simulation"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {
        "id": "setup-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install torch torchvision torchaudio --quiet\n",
        "!pip install omegaconf pyyaml boto3 psutil --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository (or upload files)\n",
        "# Uncomment and modify the URL if using a remote repository\n",
        "# !git clone https://github.com/your-org/elastic-training-harness.git\n",
        "# %cd elastic-training-harness\n",
        "\n",
        "# For this demo, we'll create the necessary files inline"
      ],
      "metadata": {
        "id": "clone-repo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "from dataclasses import dataclass, field\n",
        "from enum import Enum\n",
        "from typing import Optional, Dict, Any\n",
        "import time\n",
        "import random\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Core Components\n",
        "\n",
        "### 2.1 Learning Rate Scaling\n",
        "\n",
        "When the world size changes (workers join/leave), we need to adjust the learning rate to maintain training stability."
      ],
      "metadata": {
        "id": "lr-scaling-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScalingRule(Enum):\n",
        "    \"\"\"Learning rate scaling rules for topology changes.\"\"\"\n",
        "    LINEAR = \"linear\"  # lr *= batch_new / batch_old\n",
        "    SQRT = \"sqrt\"      # lr *= sqrt(batch_new / batch_old)\n",
        "    NONE = \"none\"      # No scaling\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ScalingConfig:\n",
        "    \"\"\"Configuration for LR scaling.\"\"\"\n",
        "    base_lr: float\n",
        "    base_batch_size: int\n",
        "    base_world_size: int\n",
        "    scaling_rule: ScalingRule = ScalingRule.LINEAR\n",
        "    warmup_steps: int = 100\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GradAccumulationConfig:\n",
        "    \"\"\"Configuration for gradient accumulation.\"\"\"\n",
        "    target_global_batch_size: int\n",
        "    local_batch_size: int\n",
        "    base_world_size: int\n",
        "\n",
        "\n",
        "class LRScalingManager:\n",
        "    \"\"\"Manages learning rate scaling during topology changes.\"\"\"\n",
        "\n",
        "    def __init__(self, config: ScalingConfig, optimizer: torch.optim.Optimizer):\n",
        "        self.config = config\n",
        "        self.optimizer = optimizer\n",
        "        self.current_world_size = config.base_world_size\n",
        "        self.current_lr = config.base_lr\n",
        "        self._warmup_step = 0\n",
        "        self._warmup_active = False\n",
        "        self._warmup_start_lr = config.base_lr\n",
        "        self._warmup_target_lr = config.base_lr\n",
        "\n",
        "    def on_topology_change(self, new_world_size: int) -> float:\n",
        "        \"\"\"Adjust learning rate when world size changes.\"\"\"\n",
        "        if new_world_size == self.current_world_size:\n",
        "            return self.current_lr\n",
        "\n",
        "        old_effective_batch = self.config.base_batch_size * self.current_world_size\n",
        "        new_effective_batch = self.config.base_batch_size * new_world_size\n",
        "\n",
        "        # Calculate new LR based on scaling rule\n",
        "        if self.config.scaling_rule == ScalingRule.LINEAR:\n",
        "            scale = new_effective_batch / old_effective_batch\n",
        "        elif self.config.scaling_rule == ScalingRule.SQRT:\n",
        "            scale = (new_effective_batch / old_effective_batch) ** 0.5\n",
        "        else:\n",
        "            scale = 1.0\n",
        "\n",
        "        new_lr = self.current_lr * scale\n",
        "\n",
        "        # Start warmup from current LR to new LR\n",
        "        self._warmup_start_lr = self.current_lr\n",
        "        self._warmup_target_lr = new_lr\n",
        "        self._warmup_step = 0\n",
        "        self._warmup_active = True\n",
        "\n",
        "        self.current_world_size = new_world_size\n",
        "        self.current_lr = new_lr\n",
        "\n",
        "        return new_lr\n",
        "\n",
        "    def step_warmup(self) -> float:\n",
        "        \"\"\"Step the warmup schedule.\"\"\"\n",
        "        if not self._warmup_active:\n",
        "            return self.current_lr\n",
        "\n",
        "        self._warmup_step += 1\n",
        "        progress = min(1.0, self._warmup_step / self.config.warmup_steps)\n",
        "\n",
        "        # Linear interpolation\n",
        "        current = self._warmup_start_lr + progress * (self._warmup_target_lr - self._warmup_start_lr)\n",
        "\n",
        "        # Update optimizer\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = current\n",
        "\n",
        "        if self._warmup_step >= self.config.warmup_steps:\n",
        "            self._warmup_active = False\n",
        "\n",
        "        return current\n",
        "\n",
        "\n",
        "class GradientAccumulationManager:\n",
        "    \"\"\"Manages gradient accumulation to maintain constant global batch size.\"\"\"\n",
        "\n",
        "    def __init__(self, config: GradAccumulationConfig):\n",
        "        self.config = config\n",
        "        self.current_world_size = config.base_world_size\n",
        "        self._micro_step = 0\n",
        "        self._accumulation_steps = self._calculate_accumulation(config.base_world_size)\n",
        "\n",
        "    def _calculate_accumulation(self, world_size: int) -> int:\n",
        "        \"\"\"Calculate accumulation steps for given world size.\"\"\"\n",
        "        per_step_batch = self.config.local_batch_size * world_size\n",
        "        return max(1, round(self.config.target_global_batch_size / per_step_batch))\n",
        "\n",
        "    def on_topology_change(self, new_world_size: int) -> int:\n",
        "        \"\"\"Adjust accumulation steps when world size changes.\"\"\"\n",
        "        self.current_world_size = new_world_size\n",
        "        self._accumulation_steps = self._calculate_accumulation(new_world_size)\n",
        "        self._micro_step = 0\n",
        "        return self._accumulation_steps\n",
        "\n",
        "    @property\n",
        "    def accumulation_steps(self) -> int:\n",
        "        return self._accumulation_steps\n",
        "\n",
        "    def should_step(self) -> bool:\n",
        "        \"\"\"Check if optimizer should step.\"\"\"\n",
        "        self._micro_step += 1\n",
        "        if self._micro_step >= self._accumulation_steps:\n",
        "            self._micro_step = 0\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "print(\"LR Scaling and Gradient Accumulation managers defined.\")"
      ],
      "metadata": {
        "id": "lr-scaling"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Checkpoint Manager\n",
        "\n",
        "Multi-tier checkpointing with memory, NVMe, and S3 backends."
      ],
      "metadata": {
        "id": "checkpoint-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CheckpointTier(Enum):\n",
        "    \"\"\"Checkpoint storage tiers in order of speed.\"\"\"\n",
        "    MEMORY = \"memory\"  # Fastest, volatile\n",
        "    NVME = \"nvme\"      # Fast, persistent\n",
        "    S3 = \"s3\"          # Slow, durable\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class CheckpointState:\n",
        "    \"\"\"Complete training state for checkpointing.\"\"\"\n",
        "    step: int\n",
        "    model_state: Dict[str, torch.Tensor]\n",
        "    optimizer_state: Dict[str, Any]\n",
        "    dataset_state: Dict[str, Any] = field(default_factory=dict)\n",
        "    world_size: int = 1\n",
        "    metrics: Dict[str, float] = field(default_factory=dict)\n",
        "    timestamp: float = field(default_factory=time.time)\n",
        "\n",
        "\n",
        "class MemorySnapshotBackend:\n",
        "    \"\"\"In-memory checkpoint storage with circular buffer.\"\"\"\n",
        "\n",
        "    def __init__(self, max_snapshots: int = 2):\n",
        "        self.max_snapshots = max_snapshots\n",
        "        self._snapshots: list = []\n",
        "\n",
        "    def save(self, state: CheckpointState) -> None:\n",
        "        \"\"\"Save snapshot to memory.\"\"\"\n",
        "        # Deep copy tensors to CPU\n",
        "        cpu_state = CheckpointState(\n",
        "            step=state.step,\n",
        "            model_state={k: v.cpu().clone() for k, v in state.model_state.items()},\n",
        "            optimizer_state=state.optimizer_state,\n",
        "            dataset_state=state.dataset_state.copy(),\n",
        "            world_size=state.world_size,\n",
        "            metrics=state.metrics.copy(),\n",
        "            timestamp=state.timestamp,\n",
        "        )\n",
        "\n",
        "        self._snapshots.append(cpu_state)\n",
        "        if len(self._snapshots) > self.max_snapshots:\n",
        "            self._snapshots.pop(0)\n",
        "\n",
        "    def load_latest(self) -> Optional[CheckpointState]:\n",
        "        \"\"\"Load most recent snapshot.\"\"\"\n",
        "        if not self._snapshots:\n",
        "            return None\n",
        "        return self._snapshots[-1]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear all snapshots.\"\"\"\n",
        "        self._snapshots.clear()\n",
        "\n",
        "\n",
        "class NVMeBackend:\n",
        "    \"\"\"Local NVMe/SSD checkpoint storage.\"\"\"\n",
        "\n",
        "    def __init__(self, base_path: str, keep_last_n: int = 3):\n",
        "        self.base_path = base_path\n",
        "        self.keep_last_n = keep_last_n\n",
        "        os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "    def save(self, state: CheckpointState) -> str:\n",
        "        \"\"\"Save checkpoint to disk.\"\"\"\n",
        "        filename = f\"checkpoint_step_{state.step}.pt\"\n",
        "        path = os.path.join(self.base_path, filename)\n",
        "\n",
        "        torch.save({\n",
        "            'step': state.step,\n",
        "            'model_state': state.model_state,\n",
        "            'optimizer_state': state.optimizer_state,\n",
        "            'dataset_state': state.dataset_state,\n",
        "            'world_size': state.world_size,\n",
        "            'metrics': state.metrics,\n",
        "            'timestamp': state.timestamp,\n",
        "        }, path)\n",
        "\n",
        "        self._cleanup_old_checkpoints()\n",
        "        return path\n",
        "\n",
        "    def load_latest(self) -> Optional[CheckpointState]:\n",
        "        \"\"\"Load most recent checkpoint.\"\"\"\n",
        "        checkpoints = sorted(\n",
        "            [f for f in os.listdir(self.base_path) if f.startswith('checkpoint_')],\n",
        "            key=lambda x: int(x.split('_')[-1].replace('.pt', '')),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        if not checkpoints:\n",
        "            return None\n",
        "\n",
        "        path = os.path.join(self.base_path, checkpoints[0])\n",
        "        data = torch.load(path, map_location='cpu')\n",
        "\n",
        "        return CheckpointState(\n",
        "            step=data['step'],\n",
        "            model_state=data['model_state'],\n",
        "            optimizer_state=data['optimizer_state'],\n",
        "            dataset_state=data.get('dataset_state', {}),\n",
        "            world_size=data.get('world_size', 1),\n",
        "            metrics=data.get('metrics', {}),\n",
        "            timestamp=data.get('timestamp', 0),\n",
        "        )\n",
        "\n",
        "    def _cleanup_old_checkpoints(self) -> None:\n",
        "        \"\"\"Remove old checkpoints beyond keep_last_n.\"\"\"\n",
        "        checkpoints = sorted(\n",
        "            [f for f in os.listdir(self.base_path) if f.startswith('checkpoint_')],\n",
        "            key=lambda x: int(x.split('_')[-1].replace('.pt', '')),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        for old_ckpt in checkpoints[self.keep_last_n:]:\n",
        "            os.remove(os.path.join(self.base_path, old_ckpt))\n",
        "\n",
        "\n",
        "print(\"Checkpoint backends defined.\")"
      ],
      "metadata": {
        "id": "checkpoint"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Simple Transformer Model"
      ],
      "metadata": {
        "id": "model-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTransformerLM(nn.Module):\n",
        "    \"\"\"Simple transformer language model for demonstration.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int = 50257,\n",
        "        d_model: int = 256,\n",
        "        nhead: int = 4,\n",
        "        num_layers: int = 2,\n",
        "        dim_feedforward: int = 512,\n",
        "        max_seq_length: int = 128,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embedding = nn.Embedding(max_seq_length, d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
        "        self.output_proj.weight = self.embedding.weight  # Tie weights\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, labels: Optional[torch.Tensor] = None):\n",
        "        seq_len = input_ids.size(1)\n",
        "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
        "\n",
        "        x = self.embedding(input_ids) + self.pos_embedding(positions)\n",
        "\n",
        "        mask = nn.Transformer.generate_square_subsequent_mask(seq_len, device=input_ids.device)\n",
        "        x = self.transformer(x, mask=mask, is_causal=True)\n",
        "\n",
        "        logits = self.output_proj(x)\n",
        "\n",
        "        output = {\"logits\": logits}\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "            output[\"loss\"] = loss\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# Test model\n",
        "model = SimpleTransformerLM()\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "metadata": {
        "id": "model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training Loop with Fault Tolerance\n",
        "\n",
        "This demonstrates a single-process training loop with:\n",
        "- Multi-tier checkpointing\n",
        "- LR scaling simulation\n",
        "- Chaos mode (random crashes)"
      ],
      "metadata": {
        "id": "training-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_fault_tolerance(\n",
        "    model: nn.Module,\n",
        "    max_steps: int = 200,\n",
        "    batch_size: int = 4,\n",
        "    seq_length: int = 128,\n",
        "    vocab_size: int = 50257,\n",
        "    checkpoint_interval: int = 50,\n",
        "    memory_snapshot_interval: int = 10,\n",
        "    chaos_enabled: bool = False,\n",
        "    chaos_probability: float = 0.02,\n",
        "    chaos_after_step: int = 30,\n",
        "):\n",
        "    \"\"\"Training loop with fault tolerance features.\"\"\"\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Setup optimizer and scaling\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "\n",
        "    lr_config = ScalingConfig(\n",
        "        base_lr=1e-4,\n",
        "        base_batch_size=batch_size,\n",
        "        base_world_size=1,\n",
        "        scaling_rule=ScalingRule.LINEAR,\n",
        "        warmup_steps=10,\n",
        "    )\n",
        "    lr_manager = LRScalingManager(lr_config, optimizer)\n",
        "\n",
        "    # Setup checkpointing\n",
        "    memory_backend = MemorySnapshotBackend(max_snapshots=2)\n",
        "    nvme_backend = NVMeBackend('/tmp/elastic_checkpoints', keep_last_n=2)\n",
        "\n",
        "    # Try to resume from checkpoint\n",
        "    start_step = 0\n",
        "    checkpoint = nvme_backend.load_latest()\n",
        "    if checkpoint:\n",
        "        model.load_state_dict(checkpoint.model_state)\n",
        "        optimizer.load_state_dict(checkpoint.optimizer_state)\n",
        "        start_step = checkpoint.step + 1\n",
        "        print(f\"Resumed from checkpoint at step {checkpoint.step}\")\n",
        "    else:\n",
        "        print(\"Starting fresh training run\")\n",
        "\n",
        "    # Synthetic data generator\n",
        "    def get_batch():\n",
        "        input_ids = torch.randint(0, vocab_size, (batch_size, seq_length), device=device)\n",
        "        labels = torch.randint(0, vocab_size, (batch_size, seq_length), device=device)\n",
        "        return input_ids, labels\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    losses = []\n",
        "    step_times = []\n",
        "\n",
        "    print(f\"\\nStarting training from step {start_step} to {max_steps}\")\n",
        "    print(f\"Chaos mode: {'ENABLED' if chaos_enabled else 'disabled'}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for step in range(start_step, max_steps):\n",
        "        step_start = time.time()\n",
        "\n",
        "        # Chaos: maybe crash (for testing recovery)\n",
        "        if chaos_enabled and step > chaos_after_step:\n",
        "            if random.random() < chaos_probability:\n",
        "                print(f\"\\nCHAOS: Simulated crash at step {step}!\")\n",
        "                print(\"Run this cell again to resume from checkpoint.\")\n",
        "                return losses, step\n",
        "\n",
        "        # Memory snapshot\n",
        "        if step > 0 and step % memory_snapshot_interval == 0:\n",
        "            state = CheckpointState(\n",
        "                step=step,\n",
        "                model_state=model.state_dict(),\n",
        "                optimizer_state=optimizer.state_dict(),\n",
        "            )\n",
        "            memory_backend.save(state)\n",
        "\n",
        "        # Get batch and forward pass\n",
        "        input_ids, labels = get_batch()\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss = outputs['loss']\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Step LR warmup\n",
        "        current_lr = lr_manager.step_warmup()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        step_times.append(time.time() - step_start)\n",
        "\n",
        "        # Logging\n",
        "        if step % 10 == 0:\n",
        "            avg_loss = sum(losses[-10:]) / min(10, len(losses))\n",
        "            avg_time = sum(step_times[-10:]) / min(10, len(step_times))\n",
        "            tokens_per_sec = batch_size * seq_length / avg_time\n",
        "            print(f\"Step {step:4d} | Loss: {avg_loss:.4f} | LR: {current_lr:.2e} | Tokens/s: {tokens_per_sec:.0f}\")\n",
        "\n",
        "        # NVMe checkpoint\n",
        "        if step > 0 and step % checkpoint_interval == 0:\n",
        "            state = CheckpointState(\n",
        "                step=step,\n",
        "                model_state=model.state_dict(),\n",
        "                optimizer_state=optimizer.state_dict(),\n",
        "                metrics={'loss': loss.item()},\n",
        "            )\n",
        "            path = nvme_backend.save(state)\n",
        "            print(f\"  [Checkpoint saved: {path}]\")\n",
        "            memory_backend.clear()  # Clear memory after persistent save\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Training complete! Final loss: {losses[-1]:.4f}\")\n",
        "    return losses, max_steps"
      ],
      "metadata": {
        "id": "training-loop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Run Training\n",
        "\n",
        "Run the training loop. If chaos mode is enabled and a crash occurs, simply run the cell again to resume from the last checkpoint."
      ],
      "metadata": {
        "id": "run-training-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "model = SimpleTransformerLM(\n",
        "    vocab_size=50257,\n",
        "    d_model=256,\n",
        "    nhead=4,\n",
        "    num_layers=2,\n",
        "    dim_feedforward=512,\n",
        "    max_seq_length=128,\n",
        ")\n",
        "\n",
        "# Run training (set chaos_enabled=True to test recovery)\n",
        "losses, final_step = train_with_fault_tolerance(\n",
        "    model,\n",
        "    max_steps=100,\n",
        "    batch_size=4,\n",
        "    seq_length=128,\n",
        "    checkpoint_interval=25,\n",
        "    memory_snapshot_interval=5,\n",
        "    chaos_enabled=False,  # Set to True to test crash recovery\n",
        "    chaos_probability=0.05,\n",
        "    chaos_after_step=20,\n",
        ")"
      ],
      "metadata": {
        "id": "run-training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Visualize Training"
      ],
      "metadata": {
        "id": "viz-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "visualize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Demonstrate Topology Change Handling\n",
        "\n",
        "This simulates what happens when workers join or leave the training cluster."
      ],
      "metadata": {
        "id": "topology-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Simulating Topology Changes\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Setup\n",
        "dummy_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "lr_config = ScalingConfig(\n",
        "    base_lr=1e-4,\n",
        "    base_batch_size=8,\n",
        "    base_world_size=4,\n",
        "    scaling_rule=ScalingRule.LINEAR,\n",
        ")\n",
        "lr_manager = LRScalingManager(lr_config, dummy_optimizer)\n",
        "\n",
        "accum_config = GradAccumulationConfig(\n",
        "    target_global_batch_size=256,\n",
        "    local_batch_size=8,\n",
        "    base_world_size=4,\n",
        ")\n",
        "accum_manager = GradientAccumulationManager(accum_config)\n",
        "\n",
        "print(f\"Initial state (4 GPUs):\")\n",
        "print(f\"  LR: {lr_manager.current_lr:.2e}\")\n",
        "print(f\"  Accumulation steps: {accum_manager.accumulation_steps}\")\n",
        "print(f\"  Global batch: {8 * 4 * accum_manager.accumulation_steps}\")\n",
        "print()\n",
        "\n",
        "# Simulate losing a GPU\n",
        "print(\"Worker failure: 4 GPUs -> 3 GPUs\")\n",
        "new_lr = lr_manager.on_topology_change(3)\n",
        "new_accum = accum_manager.on_topology_change(3)\n",
        "print(f\"  New LR: {new_lr:.2e}\")\n",
        "print(f\"  New accumulation steps: {new_accum}\")\n",
        "print(f\"  Effective global batch: {8 * 3 * new_accum}\")\n",
        "print()\n",
        "\n",
        "# Simulate adding GPUs\n",
        "print(\"Workers joining: 3 GPUs -> 6 GPUs\")\n",
        "new_lr = lr_manager.on_topology_change(6)\n",
        "new_accum = accum_manager.on_topology_change(6)\n",
        "print(f\"  New LR: {new_lr:.2e}\")\n",
        "print(f\"  New accumulation steps: {new_accum}\")\n",
        "print(f\"  Effective global batch: {8 * 6 * new_accum}\")"
      ],
      "metadata": {
        "id": "topology-demo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Multi-Process Training (Local)\n",
        "\n",
        "For actual distributed training, use the launch script. This cell shows how to run it in Colab."
      ],
      "metadata": {
        "id": "multiprocess-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a minimal training script\n",
        "training_script = '''\n",
        "import os\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "def main():\n",
        "    # Setup\n",
        "    dist.init_process_group(backend=\"gloo\")\n",
        "    rank = dist.get_rank()\n",
        "    world_size = dist.get_world_size()\n",
        "\n",
        "    print(f\"Worker {rank}/{world_size} started\")\n",
        "\n",
        "    # Simple model\n",
        "    model = nn.Linear(10, 10)\n",
        "    model = DDP(model)\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Training\n",
        "    for step in range(10):\n",
        "        x = torch.randn(4, 10)\n",
        "        loss = model(x).sum()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if rank == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    dist.destroy_process_group()\n",
        "    print(f\"Worker {rank} finished\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''\n",
        "\n",
        "with open('/tmp/distributed_demo.py', 'w') as f:\n",
        "    f.write(training_script)\n",
        "\n",
        "print(\"Distributed training script written to /tmp/distributed_demo.py\")"
      ],
      "metadata": {
        "id": "write-script"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run distributed training with 2 processes\n",
        "# Note: In Colab, we use torchrun with c10d backend (no etcd needed)\n",
        "!torchrun --standalone --nproc-per-node=2 /tmp/distributed_demo.py"
      ],
      "metadata": {
        "id": "run-distributed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. **LR Scaling** - Automatically adjust learning rate when topology changes\n",
        "2. **Gradient Accumulation** - Maintain constant global batch size\n",
        "3. **Multi-tier Checkpointing** - Memory and NVMe backends with automatic cleanup\n",
        "4. **Fault Tolerance** - Resume from checkpoint after simulated crashes\n",
        "5. **Distributed Training** - Using torchrun with c10d backend\n",
        "\n",
        "For production use:\n",
        "- Use the full `elastic_harness` package\n",
        "- Deploy etcd for multi-node rendezvous\n",
        "- Configure S3 backend for durable checkpoints\n",
        "- Enable chaos testing to validate fault tolerance"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}
