{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Large-Scale Elastic Training Practice\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mmcmanus1/elastic-training-harness/blob/main/notebooks/elastic_training_large.ipynb)\n\nA comprehensive practice notebook with:\n- **~100M parameter model** (3.5x larger than basic demo)\n- **Real data** - WikiText-2 dataset with GPT-2 tokenization\n- **500 training steps** with validation tracking\n- **Full feature demonstration** - checkpointing, scaling, recovery\n\nDesigned for Google Colab T4 GPU (16GB VRAM)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Clone the repository and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup: Clone and install elastic-training-harness\n# Clean up any existing installation (safe to re-run)\n!rm -rf /content/elastic-training-harness\n\n# Clone the repository\n!git clone https://github.com/mmcmanus1/elastic-training-harness.git\n\n# Install the package and additional dependencies (non-editable for Colab compatibility)\n%cd /content/elastic-training-harness\n!pip install . --quiet\n!pip install datasets transformers --quiet\n%cd /content\n\n# Force Python to recognize newly installed package\nimport importlib\nimport sys\nif 'elastic_harness' in sys.modules:\n    del sys.modules['elastic_harness']\nimportlib.invalidate_caches()\n\n# Verify installation\ntry:\n    import elastic_harness\n    print(f\"elastic_harness {elastic_harness.__version__} installed successfully!\")\nexcept ImportError as e:\n    print(f\"Installation failed: {e}\")\n    print(\"Try: Runtime -> Restart runtime, then run this cell again\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {gpu_memory:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import elastic_harness Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint management\n",
    "from elastic_harness.checkpoint import (\n",
    "    CheckpointConfig,\n",
    "    CheckpointState,\n",
    "    CheckpointManager,\n",
    "    CheckpointTier,\n",
    "    MemorySnapshotBackend,\n",
    "    NVMeBackend,\n",
    ")\n",
    "\n",
    "# LR scaling and gradient accumulation\n",
    "from elastic_harness.scaling import (\n",
    "    ScalingRule,\n",
    "    ScalingConfig,\n",
    "    LRScalingManager,\n",
    "    GradAccumulationConfig,\n",
    "    GradientAccumulationManager,\n",
    "    ElasticScalingManager,\n",
    ")\n",
    "\n",
    "print(\"elastic_harness components imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load WikiText-2 Dataset\n",
    "\n",
    "Load real text data and tokenize with GPT-2 tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load WikiText-2\n",
    "print(\"Loading WikiText-2 dataset...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 256\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    \"\"\"Dataset that tokenizes and chunks text into fixed-length sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, tokenizer, seq_length):\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Concatenate all text and tokenize\n",
    "        all_text = \" \".join([t for t in texts if t.strip()])\n",
    "        tokens = tokenizer.encode(all_text, add_special_tokens=False)\n",
    "        \n",
    "        # Chunk into sequences\n",
    "        self.chunks = []\n",
    "        for i in range(0, len(tokens) - seq_length, seq_length):\n",
    "            self.chunks.append(tokens[i:i + seq_length])\n",
    "        \n",
    "        print(f\"Created {len(self.chunks)} sequences of length {seq_length}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = torch.tensor(self.chunks[idx], dtype=torch.long)\n",
    "        # Input is all but last token, labels shifted by 1\n",
    "        return chunk[:-1], chunk[1:]\n",
    "\n",
    "# Create datasets\n",
    "print(\"Tokenizing and chunking train data...\")\n",
    "train_dataset = WikiTextDataset(\n",
    "    dataset['train']['text'], \n",
    "    tokenizer, \n",
    "    SEQ_LENGTH + 1  # +1 for labels shift\n",
    ")\n",
    "\n",
    "print(\"Tokenizing and chunking validation data...\")\n",
    "val_dataset = WikiTextDataset(\n",
    "    dataset['validation']['text'],\n",
    "    tokenizer,\n",
    "    SEQ_LENGTH + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "BATCH_SIZE = 2  # Small batch, will use gradient accumulation\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define ~100M Parameter Model\n",
    "\n",
    "Larger transformer model sized for T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeTransformerLM(nn.Module):\n",
    "    \"\"\"~100M parameter transformer language model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 50257,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        num_layers: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        max_seq_length: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_length, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True,  # Pre-norm for better training\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer, \n",
    "            num_layers=num_layers,\n",
    "            enable_nested_tensor=False,\n",
    "        )\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.output_proj.weight = self.embedding.weight  # Tie weights\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, labels=None):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Create position indices\n",
    "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
    "        positions = positions.expand(batch_size, -1)\n",
    "\n",
    "        # Embeddings\n",
    "        x = self.embedding(input_ids) + self.pos_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Causal mask\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(\n",
    "            seq_len, device=input_ids.device\n",
    "        )\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.transformer(x, mask=mask, is_causal=True)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        # Output projection\n",
    "        logits = self.output_proj(x)\n",
    "\n",
    "        output = {\"logits\": logits}\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            output[\"loss\"] = loss\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Create model and count parameters\n",
    "model = LargeTransformerLM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    max_seq_length=SEQ_LENGTH,\n",
    ")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "print(f\"Trainable parameters: {num_trainable:,}\")\n",
    "print(f\"Model size: ~{num_params * 4 / 1e6:.1f} MB (fp32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training with Full elastic_harness Features\n",
    "\n",
    "Training loop with:\n",
    "- Gradient accumulation (effective batch size 8)\n",
    "- Multi-tier checkpointing (Memory + NVMe)\n",
    "- Validation tracking\n",
    "- Memory monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_mb():\n",
    "    \"\"\"Get current GPU memory usage in MB.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1e6\n",
    "    return 0\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, device, max_batches=50):\n",
    "    \"\"\"Run validation and return average loss.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for i, (input_ids, labels) in enumerate(val_loader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        total_loss += outputs['loss'].item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    model.train()\n",
    "    return total_loss / num_batches if num_batches > 0 else 0\n",
    "\n",
    "\n",
    "def train_large_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    max_steps: int = 500,\n",
    "    accumulation_steps: int = 4,  # Effective batch = 2 * 4 = 8\n",
    "    checkpoint_interval: int = 100,\n",
    "    memory_snapshot_interval: int = 25,\n",
    "    val_interval: int = 100,\n",
    "    log_interval: int = 25,\n",
    "    chaos_enabled: bool = False,\n",
    "    chaos_probability: float = 0.01,\n",
    "    chaos_after_step: int = 200,\n",
    "):\n",
    "    \"\"\"Training loop with full elastic_harness features.\"\"\"\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=3e-4,  # Higher LR for larger model\n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.95),\n",
    "    )\n",
    "\n",
    "    # Setup elastic scaling manager (unified LR + accumulation)\n",
    "    lr_config = ScalingConfig(\n",
    "        base_lr=3e-4,\n",
    "        base_batch_size=BATCH_SIZE * accumulation_steps,\n",
    "        base_world_size=1,\n",
    "        scaling_rule=ScalingRule.SQRT,  # Sqrt scaling for large batches\n",
    "        warmup_steps=50,\n",
    "    )\n",
    "    lr_manager = LRScalingManager(lr_config, optimizer)\n",
    "\n",
    "    # Setup multi-tier checkpoint backends\n",
    "    memory_backend = MemorySnapshotBackend(max_snapshots=3)\n",
    "    nvme_backend = NVMeBackend(base_path='/tmp/elastic_large_checkpoints')\n",
    "\n",
    "    # Try to resume from checkpoint\n",
    "    start_step = 0\n",
    "    latest_path = nvme_backend.get_latest_checkpoint()\n",
    "    if latest_path:\n",
    "        checkpoint = nvme_backend.load(latest_path)\n",
    "        model.load_state_dict(checkpoint.model_state_dict)\n",
    "        optimizer.load_state_dict(checkpoint.optimizer_state_dict)\n",
    "        start_step = checkpoint.step + 1\n",
    "        print(f\"Resumed from checkpoint at step {checkpoint.step}\")\n",
    "    else:\n",
    "        print(\"Starting fresh training run\")\n",
    "\n",
    "    # Training state\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_steps = []\n",
    "    memory_usage = []\n",
    "    \n",
    "    model.train()\n",
    "    train_iter = iter(train_loader)\n",
    "    accumulated_loss = 0\n",
    "    \n",
    "    print(f\"\\nStarting training from step {start_step} to {max_steps}\")\n",
    "    print(f\"Effective batch size: {BATCH_SIZE * accumulation_steps}\")\n",
    "    print(f\"Chaos mode: {'ENABLED' if chaos_enabled else 'disabled'}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for step in range(start_step, max_steps):\n",
    "        step_start = time.time()\n",
    "\n",
    "        # Chaos: maybe crash (for testing recovery)\n",
    "        if chaos_enabled and step > chaos_after_step:\n",
    "            if random.random() < chaos_probability:\n",
    "                print(f\"\\nCHAOS: Simulated crash at step {step}!\")\n",
    "                print(\"Run this cell again to resume from checkpoint.\")\n",
    "                return {\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'val_steps': val_steps,\n",
    "                    'memory_usage': memory_usage,\n",
    "                    'final_step': step,\n",
    "                    'crashed': True,\n",
    "                }\n",
    "\n",
    "        # Memory snapshot\n",
    "        if step > 0 and step % memory_snapshot_interval == 0:\n",
    "            state = CheckpointState(\n",
    "                step=step,\n",
    "                model_state_dict=model.state_dict(),\n",
    "                optimizer_state_dict=optimizer.state_dict(),\n",
    "            )\n",
    "            memory_backend.save(state)\n",
    "\n",
    "        # Gradient accumulation loop\n",
    "        optimizer.zero_grad()\n",
    "        for micro_step in range(accumulation_steps):\n",
    "            try:\n",
    "                input_ids, labels = next(train_iter)\n",
    "            except StopIteration:\n",
    "                train_iter = iter(train_loader)\n",
    "                input_ids, labels = next(train_iter)\n",
    "            \n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(input_ids, labels=labels)\n",
    "            loss = outputs['loss'] / accumulation_steps\n",
    "            loss.backward()\n",
    "            accumulated_loss += loss.item()\n",
    "\n",
    "        # Gradient clipping and optimizer step\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # LR warmup step\n",
    "        current_lr = lr_manager.step()\n",
    "\n",
    "        train_losses.append(accumulated_loss)\n",
    "        memory_usage.append(get_gpu_memory_mb())\n",
    "        accumulated_loss = 0\n",
    "\n",
    "        # Logging\n",
    "        if step % log_interval == 0:\n",
    "            avg_loss = sum(train_losses[-log_interval:]) / min(log_interval, len(train_losses))\n",
    "            mem_mb = memory_usage[-1]\n",
    "            step_time = time.time() - step_start\n",
    "            tokens_per_sec = BATCH_SIZE * accumulation_steps * SEQ_LENGTH / step_time\n",
    "            print(\n",
    "                f\"Step {step:4d} | Loss: {avg_loss:.4f} | \"\n",
    "                f\"LR: {current_lr:.2e} | Mem: {mem_mb:.0f}MB | \"\n",
    "                f\"Tok/s: {tokens_per_sec:.0f}\"\n",
    "            )\n",
    "\n",
    "        # Validation\n",
    "        if step > 0 and step % val_interval == 0:\n",
    "            val_loss = evaluate(model, val_loader, device)\n",
    "            val_losses.append(val_loss)\n",
    "            val_steps.append(step)\n",
    "            print(f\"  [Validation loss: {val_loss:.4f}]\")\n",
    "\n",
    "        # NVMe checkpoint\n",
    "        if step > 0 and step % checkpoint_interval == 0:\n",
    "            state = CheckpointState(\n",
    "                step=step,\n",
    "                model_state_dict=model.state_dict(),\n",
    "                optimizer_state_dict=optimizer.state_dict(),\n",
    "                metrics={'train_loss': train_losses[-1], 'val_loss': val_losses[-1] if val_losses else None},\n",
    "            )\n",
    "            path = f\"checkpoint_step_{step:08d}.pt\"\n",
    "            nvme_backend.save(state, path)\n",
    "            nvme_backend.cleanup_old_checkpoints(keep_last=2)\n",
    "            print(f\"  [Checkpoint saved: {path}]\")\n",
    "            memory_backend.clear()\n",
    "\n",
    "    # Final validation\n",
    "    final_val_loss = evaluate(model, val_loader, device)\n",
    "    val_losses.append(final_val_loss)\n",
    "    val_steps.append(max_steps)\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Training complete!\")\n",
    "    print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"Final val loss: {final_val_loss:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_steps': val_steps,\n",
    "        'memory_usage': memory_usage,\n",
    "        'final_step': max_steps,\n",
    "        'crashed': False,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Training\n",
    "\n",
    "Train for 500 steps. Set `chaos_enabled=True` to test crash recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fresh model\n",
    "model = LargeTransformerLM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    max_seq_length=SEQ_LENGTH,\n",
    ")\n",
    "\n",
    "# Run training\n",
    "results = train_large_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    max_steps=500,\n",
    "    accumulation_steps=4,  # Effective batch = 8\n",
    "    checkpoint_interval=100,\n",
    "    memory_snapshot_interval=25,\n",
    "    val_interval=100,\n",
    "    log_interval=25,\n",
    "    chaos_enabled=False,  # Set True to test recovery\n",
    "    chaos_probability=0.02,\n",
    "    chaos_after_step=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Training loss\n",
    "axes[0].plot(results['train_losses'], alpha=0.7)\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation loss\n",
    "axes[1].plot(results['val_steps'], results['val_losses'], 'o-', color='orange')\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Validation Loss')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Memory usage\n",
    "axes[2].plot(results['memory_usage'], color='green', alpha=0.7)\n",
    "axes[2].set_xlabel('Step')\n",
    "axes[2].set_ylabel('Memory (MB)')\n",
    "axes[2].set_title('GPU Memory Usage')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial loss: {results['train_losses'][0]:.4f}\")\n",
    "print(f\"Final loss: {results['train_losses'][-1]:.4f}\")\n",
    "print(f\"Loss reduction: {results['train_losses'][0] - results['train_losses'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Checkpoint Recovery Demo\n",
    "\n",
    "Demonstrate loading from a saved checkpoint and continuing training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checkpoint Recovery Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a fresh model\n",
    "fresh_model = LargeTransformerLM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    max_seq_length=SEQ_LENGTH,\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "nvme_backend = NVMeBackend(base_path='/tmp/elastic_large_checkpoints')\n",
    "latest_path = nvme_backend.get_latest_checkpoint()\n",
    "\n",
    "if latest_path:\n",
    "    checkpoint = nvme_backend.load(latest_path)\n",
    "    fresh_model.load_state_dict(checkpoint.model_state_dict)\n",
    "    \n",
    "    print(f\"Loaded checkpoint from step {checkpoint.step}\")\n",
    "    print(f\"Checkpoint metrics: {checkpoint.metrics}\")\n",
    "    \n",
    "    # Verify model works\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    fresh_model = fresh_model.to(device)\n",
    "    \n",
    "    val_loss = evaluate(fresh_model, val_loader, device)\n",
    "    print(f\"Validation loss after loading: {val_loss:.4f}\")\n",
    "    print(\"\\nCheckpoint recovery successful!\")\n",
    "else:\n",
    "    print(\"No checkpoint found - run training first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Topology Change Simulation\n",
    "\n",
    "Demonstrate how elastic_harness handles dynamic world size changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Simulating Topology Changes\")\nprint(\"=\" * 60)\n\n# Setup with elastic scaling managers\ndummy_model = LargeTransformerLM(vocab_size=1000, d_model=64, num_layers=1)  # Tiny for demo\ndummy_optimizer = torch.optim.AdamW(dummy_model.parameters(), lr=3e-4)\n\n# ElasticScalingManager combines LR and accumulation\nelastic_manager = ElasticScalingManager(\n    lr_config=ScalingConfig(\n        base_lr=3e-4,\n        base_batch_size=8,\n        base_world_size=4,\n        scaling_rule=ScalingRule.SQRT,\n        warmup_steps=100,\n    ),\n    accum_config=GradAccumulationConfig(\n        target_global_batch_size=512,\n        local_batch_size=8,\n        base_world_size=4,\n    ),\n    optimizer=dummy_optimizer,\n)\n\nprint(f\"Initial state (4 GPUs):\")\nprint(f\"  LR: {elastic_manager.current_lr:.2e}\")\nprint(f\"  Accumulation steps: {elastic_manager.accumulation_steps}\")\nprint(f\"  Global batch: {8 * 4 * elastic_manager.accumulation_steps}\")\nprint()\n\n# Simulate losing 2 GPUs\nprint(\"Worker failure: 4 GPUs -> 2 GPUs\")\nelastic_manager.on_topology_change(2)\nprint(f\"  New LR: {elastic_manager.current_lr:.2e}\")\nprint(f\"  New accumulation: {elastic_manager.accumulation_steps}\")\nprint(f\"  Global batch: {8 * 2 * elastic_manager.accumulation_steps}\")\nprint()\n\n# Simulate scaling up to 8 GPUs\nprint(\"Scaling up: 2 GPUs -> 8 GPUs\")\nelastic_manager.on_topology_change(8)\nprint(f\"  New LR: {elastic_manager.current_lr:.2e}\")\nprint(f\"  New accumulation: {elastic_manager.accumulation_steps}\")\nprint(f\"  Global batch: {8 * 8 * elastic_manager.accumulation_steps}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Sample Text\n",
    "\n",
    "Use the trained model to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=50, temperature=0.8, device='cuda'):\n",
    "    \"\"\"Simple greedy/sampling text generation.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Truncate if too long\n",
    "        if input_ids.size(1) >= model.max_seq_length:\n",
    "            input_ids = input_ids[:, -model.max_seq_length + 1:]\n",
    "        \n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs['logits'][:, -1, :] / temperature\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        \n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Generate some text\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "prompts = [\n",
    "    \"The history of\",\n",
    "    \"In the year\",\n",
    "    \"Scientists discovered that\",\n",
    "]\n",
    "\n",
    "print(\"Text Generation Examples\")\n",
    "print(\"=\" * 60)\n",
    "for prompt in prompts:\n",
    "    generated = generate(model, tokenizer, prompt, max_new_tokens=30, temperature=0.9, device=device)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated large-scale training with `elastic_harness`:\n",
    "\n",
    "| Feature | Details |\n",
    "|---------|--------|\n",
    "| Model | ~100M parameters (512d, 6 layers, 8 heads) |\n",
    "| Data | WikiText-2 with GPT-2 tokenization |\n",
    "| Training | 500 steps, batch 8 (via accumulation) |\n",
    "| Checkpointing | Memory + NVMe multi-tier |\n",
    "| Scaling | SQRT rule, 50-step warmup |\n",
    "| Validation | Every 100 steps |\n",
    "\n",
    "**Key elastic_harness components used:**\n",
    "- `MemorySnapshotBackend` - Fast in-memory checkpoints\n",
    "- `NVMeBackend` - Persistent disk checkpoints with cleanup\n",
    "- `LRScalingManager` - Learning rate scaling on topology changes\n",
    "- `ElasticScalingManager` - Unified LR + accumulation management\n",
    "\n",
    "**For production deployment:**\n",
    "- Add `S3Backend` for durable cloud checkpoints\n",
    "- Use `CheckpointManager` for coordinated multi-tier saves\n",
    "- Deploy etcd for multi-node elastic rendezvous\n",
    "- Enable chaos testing to validate fault tolerance"
   ]
  }
 ]
}