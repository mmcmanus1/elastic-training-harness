{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Fine-Tuning with Elastic Training\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mmcmanus1/elastic-training-harness/blob/main/notebooks/elastic_training_gpt2_finetune.ipynb)\n",
    "\n",
    "A complete fine-tuning pipeline that produces **coherent text output**:\n",
    "- **Pretrained GPT-2** (124M parameters) as starting point\n",
    "- **WikiText-2 dataset** for domain adaptation\n",
    "- **1000 training steps** with validation tracking\n",
    "- **Full elastic_harness features** - checkpointing, scaling, recovery\n",
    "\n",
    "Unlike the basic demo notebooks that train from scratch (and produce gibberish), this notebook fine-tunes a pretrained model to demonstrate a full training pipeline with coherent results.\n",
    "\n",
    "Designed for Google Colab T4 GPU (16GB VRAM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Clone the repository and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Clone and install elastic-training-harness\n",
    "# Clean up any existing installation (safe to re-run)\n",
    "!rm -rf /content/elastic-training-harness\n",
    "\n",
    "# Clone the repository\n",
    "!git clone https://github.com/mmcmanus1/elastic-training-harness.git\n",
    "\n",
    "# Install the package and additional dependencies (non-editable for Colab compatibility)\n",
    "%cd /content/elastic-training-harness\n",
    "!pip install . --quiet\n",
    "!pip install datasets transformers --quiet\n",
    "%cd /content\n",
    "\n",
    "# Force Python to recognize newly installed package\n",
    "import importlib\n",
    "import sys\n",
    "if 'elastic_harness' in sys.modules:\n",
    "    del sys.modules['elastic_harness']\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    import elastic_harness\n",
    "    print(f\"elastic_harness {elastic_harness.__version__} installed successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Installation failed: {e}\")\n",
    "    print(\"Try: Runtime -> Restart runtime, then run this cell again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {gpu_memory:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import elastic_harness Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint management\n",
    "from elastic_harness.checkpoint import (\n",
    "    CheckpointConfig,\n",
    "    CheckpointState,\n",
    "    CheckpointManager,\n",
    "    CheckpointTier,\n",
    "    MemorySnapshotBackend,\n",
    "    NVMeBackend,\n",
    ")\n",
    "\n",
    "# LR scaling and gradient accumulation\n",
    "from elastic_harness.scaling import (\n",
    "    ScalingRule,\n",
    "    ScalingConfig,\n",
    "    LRScalingManager,\n",
    "    GradAccumulationConfig,\n",
    "    GradientAccumulationManager,\n",
    "    ElasticScalingManager,\n",
    ")\n",
    "\n",
    "print(\"elastic_harness components imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Pretrained GPT-2\n",
    "\n",
    "Load GPT-2 (124M parameters) from Hugging Face. This model was pretrained on ~40GB of internet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pretrained GPT-2\n",
    "print(\"Loading pretrained GPT-2...\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "print(f\"Trainable parameters: {num_trainable:,}\")\n",
    "print(f\"Model size: ~{num_params * 4 / 1e6:.1f} MB (fp32)\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load WikiText-2 Dataset\n",
    "\n",
    "Fine-tune on WikiText-2 to adapt the model to Wikipedia-style text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load WikiText-2\n",
    "print(\"Loading WikiText-2 dataset...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 256\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    \"\"\"Dataset that tokenizes and chunks text into fixed-length sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, tokenizer, seq_length):\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Concatenate all text and tokenize\n",
    "        all_text = \" \".join([t for t in texts if t.strip()])\n",
    "        tokens = tokenizer.encode(all_text, add_special_tokens=False)\n",
    "        \n",
    "        # Chunk into sequences\n",
    "        self.chunks = []\n",
    "        for i in range(0, len(tokens) - seq_length, seq_length):\n",
    "            self.chunks.append(tokens[i:i + seq_length])\n",
    "        \n",
    "        print(f\"Created {len(self.chunks)} sequences of length {seq_length}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = torch.tensor(self.chunks[idx], dtype=torch.long)\n",
    "        # For causal LM, input and labels are the same (model handles shifting)\n",
    "        return chunk, chunk\n",
    "\n",
    "# Create datasets\n",
    "print(\"Tokenizing and chunking train data...\")\n",
    "train_dataset = WikiTextDataset(\n",
    "    dataset['train']['text'], \n",
    "    tokenizer, \n",
    "    SEQ_LENGTH\n",
    ")\n",
    "\n",
    "print(\"Tokenizing and chunking validation data...\")\n",
    "val_dataset = WikiTextDataset(\n",
    "    dataset['validation']['text'],\n",
    "    tokenizer,\n",
    "    SEQ_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "BATCH_SIZE = 2  # Small batch, will use gradient accumulation\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Generation Function\n",
    "\n",
    "Define a function to generate text and track improvement during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_samples(model, tokenizer, prompts, device, max_new_tokens=50, temperature=0.8):\n",
    "    \"\"\"Generate text samples from the model.\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "        \n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        generated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        results.append((prompt, generated))\n",
    "    \n",
    "    model.train()\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_samples(samples, step_label):\n",
    "    \"\"\"Pretty print generated samples.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Text Generation - {step_label}\")\n",
    "    print('='*60)\n",
    "    for prompt, generated in samples:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated: {generated}\")\n",
    "\n",
    "\n",
    "# Test prompts we'll use throughout training\n",
    "TEST_PROMPTS = [\n",
    "    \"The history of\",\n",
    "    \"In the year 2024,\",\n",
    "    \"Scientists discovered that\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline Generation (Before Fine-Tuning)\n",
    "\n",
    "Generate text with the pretrained model before any fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Generate baseline samples\n",
    "baseline_samples = generate_samples(model, tokenizer, TEST_PROMPTS, device)\n",
    "print_samples(baseline_samples, \"Pretrained Baseline (Step 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training with Full elastic_harness Features\n",
    "\n",
    "Fine-tuning loop with:\n",
    "- Gradient accumulation (effective batch size 8)\n",
    "- Multi-tier checkpointing (Memory + NVMe)\n",
    "- LR warmup with scaling\n",
    "- Validation and perplexity tracking\n",
    "- Text generation checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_mb():\n",
    "    \"\"\"Get current GPU memory usage in MB.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1e6\n",
    "    return 0\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, device, max_batches=50):\n",
    "    \"\"\"Run validation and return average loss and perplexity.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for i, (input_ids, labels) in enumerate(val_loader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        total_loss += outputs.loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    perplexity = math.exp(avg_loss) if avg_loss < 10 else float('inf')\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "\n",
    "def finetune_gpt2(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    tokenizer,\n",
    "    max_steps: int = 1000,\n",
    "    accumulation_steps: int = 4,  # Effective batch = 2 * 4 = 8\n",
    "    checkpoint_interval: int = 200,\n",
    "    memory_snapshot_interval: int = 25,\n",
    "    val_interval: int = 100,\n",
    "    log_interval: int = 25,\n",
    "    generation_interval: int = 500,\n",
    "    chaos_enabled: bool = False,\n",
    "    chaos_probability: float = 0.01,\n",
    "    chaos_after_step: int = 300,\n",
    "):\n",
    "    \"\"\"Fine-tuning loop with full elastic_harness features.\"\"\"\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Setup optimizer with fine-tuning learning rate\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=5e-5,  # Lower LR for fine-tuning\n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.999),\n",
    "    )\n",
    "\n",
    "    # Setup elastic scaling manager (unified LR + accumulation)\n",
    "    lr_config = ScalingConfig(\n",
    "        base_lr=5e-5,\n",
    "        base_batch_size=BATCH_SIZE * accumulation_steps,\n",
    "        base_world_size=1,\n",
    "        scaling_rule=ScalingRule.SQRT,\n",
    "        warmup_steps=100,  # Longer warmup for fine-tuning\n",
    "    )\n",
    "    lr_manager = LRScalingManager(lr_config, optimizer)\n",
    "\n",
    "    # Setup multi-tier checkpoint backends\n",
    "    memory_backend = MemorySnapshotBackend(max_snapshots=3)\n",
    "    nvme_backend = NVMeBackend(base_path='/tmp/gpt2_finetune_checkpoints')\n",
    "\n",
    "    # Try to resume from checkpoint\n",
    "    start_step = 0\n",
    "    latest_path = nvme_backend.get_latest_checkpoint()\n",
    "    if latest_path:\n",
    "        checkpoint = nvme_backend.load(latest_path)\n",
    "        model.load_state_dict(checkpoint.model_state_dict)\n",
    "        optimizer.load_state_dict(checkpoint.optimizer_state_dict)\n",
    "        start_step = checkpoint.step + 1\n",
    "        print(f\"Resumed from checkpoint at step {checkpoint.step}\")\n",
    "    else:\n",
    "        print(\"Starting fresh fine-tuning run\")\n",
    "\n",
    "    # Training state\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_perplexities = []\n",
    "    val_steps = []\n",
    "    memory_usage = []\n",
    "    generation_samples = {}  # Store samples at different steps\n",
    "    \n",
    "    model.train()\n",
    "    train_iter = iter(train_loader)\n",
    "    accumulated_loss = 0\n",
    "    \n",
    "    print(f\"\\nStarting fine-tuning from step {start_step} to {max_steps}\")\n",
    "    print(f\"Effective batch size: {BATCH_SIZE * accumulation_steps}\")\n",
    "    print(f\"Learning rate: 5e-5 (with {lr_config.warmup_steps}-step warmup)\")\n",
    "    print(f\"Chaos mode: {'ENABLED' if chaos_enabled else 'disabled'}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for step in range(start_step, max_steps):\n",
    "        step_start = time.time()\n",
    "\n",
    "        # Chaos: maybe crash (for testing recovery)\n",
    "        if chaos_enabled and step > chaos_after_step:\n",
    "            if random.random() < chaos_probability:\n",
    "                print(f\"\\nCHAOS: Simulated crash at step {step}!\")\n",
    "                print(\"Run this cell again to resume from checkpoint.\")\n",
    "                return {\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'val_perplexities': val_perplexities,\n",
    "                    'val_steps': val_steps,\n",
    "                    'memory_usage': memory_usage,\n",
    "                    'generation_samples': generation_samples,\n",
    "                    'final_step': step,\n",
    "                    'crashed': True,\n",
    "                }\n",
    "\n",
    "        # Memory snapshot\n",
    "        if step > 0 and step % memory_snapshot_interval == 0:\n",
    "            state = CheckpointState(\n",
    "                step=step,\n",
    "                model_state_dict=model.state_dict(),\n",
    "                optimizer_state_dict=optimizer.state_dict(),\n",
    "            )\n",
    "            memory_backend.save(state)\n",
    "\n",
    "        # Gradient accumulation loop\n",
    "        optimizer.zero_grad()\n",
    "        for micro_step in range(accumulation_steps):\n",
    "            try:\n",
    "                input_ids, labels = next(train_iter)\n",
    "            except StopIteration:\n",
    "                train_iter = iter(train_loader)\n",
    "                input_ids, labels = next(train_iter)\n",
    "            \n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            loss = outputs.loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            accumulated_loss += loss.item()\n",
    "\n",
    "        # Gradient clipping and optimizer step\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # LR warmup step\n",
    "        current_lr = lr_manager.step()\n",
    "\n",
    "        train_losses.append(accumulated_loss)\n",
    "        memory_usage.append(get_gpu_memory_mb())\n",
    "        accumulated_loss = 0\n",
    "\n",
    "        # Logging\n",
    "        if step % log_interval == 0:\n",
    "            avg_loss = sum(train_losses[-log_interval:]) / min(log_interval, len(train_losses))\n",
    "            ppl = math.exp(avg_loss) if avg_loss < 10 else float('inf')\n",
    "            mem_mb = memory_usage[-1]\n",
    "            step_time = time.time() - step_start\n",
    "            tokens_per_sec = BATCH_SIZE * accumulation_steps * SEQ_LENGTH / step_time\n",
    "            print(\n",
    "                f\"Step {step:4d} | Loss: {avg_loss:.4f} | PPL: {ppl:.1f} | \"\n",
    "                f\"LR: {current_lr:.2e} | Mem: {mem_mb:.0f}MB | \"\n",
    "                f\"Tok/s: {tokens_per_sec:.0f}\"\n",
    "            )\n",
    "\n",
    "        # Validation\n",
    "        if step > 0 and step % val_interval == 0:\n",
    "            val_loss, val_ppl = evaluate(model, val_loader, device)\n",
    "            val_losses.append(val_loss)\n",
    "            val_perplexities.append(val_ppl)\n",
    "            val_steps.append(step)\n",
    "            print(f\"  [Validation loss: {val_loss:.4f}, perplexity: {val_ppl:.1f}]\")\n",
    "\n",
    "        # Generation checkpoint\n",
    "        if step > 0 and step % generation_interval == 0:\n",
    "            samples = generate_samples(model, tokenizer, TEST_PROMPTS, device)\n",
    "            generation_samples[step] = samples\n",
    "            print_samples(samples, f\"Step {step}\")\n",
    "            model.train()  # Ensure back in training mode\n",
    "\n",
    "        # NVMe checkpoint\n",
    "        if step > 0 and step % checkpoint_interval == 0:\n",
    "            state = CheckpointState(\n",
    "                step=step,\n",
    "                model_state_dict=model.state_dict(),\n",
    "                optimizer_state_dict=optimizer.state_dict(),\n",
    "                metrics={'train_loss': train_losses[-1], 'val_loss': val_losses[-1] if val_losses else None},\n",
    "            )\n",
    "            path = f\"checkpoint_step_{step:08d}.pt\"\n",
    "            nvme_backend.save(state, path)\n",
    "            nvme_backend.cleanup_old_checkpoints(keep_last=2)\n",
    "            print(f\"  [Checkpoint saved: {path}]\")\n",
    "            memory_backend.clear()\n",
    "\n",
    "    # Final validation\n",
    "    final_val_loss, final_val_ppl = evaluate(model, val_loader, device)\n",
    "    val_losses.append(final_val_loss)\n",
    "    val_perplexities.append(final_val_ppl)\n",
    "    val_steps.append(max_steps)\n",
    "\n",
    "    # Final generation\n",
    "    final_samples = generate_samples(model, tokenizer, TEST_PROMPTS, device)\n",
    "    generation_samples[max_steps] = final_samples\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Fine-tuning complete!\")\n",
    "    print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"Final val loss: {final_val_loss:.4f}\")\n",
    "    print(f\"Final perplexity: {final_val_ppl:.1f}\")\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_perplexities': val_perplexities,\n",
    "        'val_steps': val_steps,\n",
    "        'memory_usage': memory_usage,\n",
    "        'generation_samples': generation_samples,\n",
    "        'final_step': max_steps,\n",
    "        'crashed': False,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Fine-Tuning\n",
    "\n",
    "Fine-tune for 1000 steps. Set `chaos_enabled=True` to test crash recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run fine-tuning\n",
    "results = finetune_gpt2(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    tokenizer,\n",
    "    max_steps=1000,\n",
    "    accumulation_steps=4,  # Effective batch = 8\n",
    "    checkpoint_interval=200,\n",
    "    memory_snapshot_interval=25,\n",
    "    val_interval=100,\n",
    "    log_interval=25,\n",
    "    generation_interval=500,\n",
    "    chaos_enabled=False,  # Set True to test recovery\n",
    "    chaos_probability=0.02,\n",
    "    chaos_after_step=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Training loss\n",
    "axes[0, 0].plot(results['train_losses'], alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation loss\n",
    "axes[0, 1].plot(results['val_steps'], results['val_losses'], 'o-', color='orange')\n",
    "axes[0, 1].set_xlabel('Step')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].set_title('Validation Loss')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Perplexity\n",
    "axes[1, 0].plot(results['val_steps'], results['val_perplexities'], 's-', color='green')\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 0].set_ylabel('Perplexity')\n",
    "axes[1, 0].set_title('Validation Perplexity')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Memory usage\n",
    "axes[1, 1].plot(results['memory_usage'], color='purple', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Step')\n",
    "axes[1, 1].set_ylabel('Memory (MB)')\n",
    "axes[1, 1].set_title('GPU Memory Usage')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial loss: {results['train_losses'][0]:.4f}\")\n",
    "print(f\"Final loss: {results['train_losses'][-1]:.4f}\")\n",
    "print(f\"Loss reduction: {results['train_losses'][0] - results['train_losses'][-1]:.4f}\")\n",
    "if results['val_perplexities']:\n",
    "    print(f\"Final perplexity: {results['val_perplexities'][-1]:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Text Generation Comparison\n",
    "\n",
    "Compare text generation before and after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TEXT GENERATION COMPARISON: Before vs After Fine-Tuning\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"BASELINE (Pretrained GPT-2, before fine-tuning)\")\n",
    "print(\"-\"*70)\n",
    "for prompt, generated in baseline_samples:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(f\"AFTER FINE-TUNING (Step {results['final_step']})\")\n",
    "print(\"-\"*70)\n",
    "final_samples = results['generation_samples'].get(results['final_step'], [])\n",
    "for prompt, generated in final_samples:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"The fine-tuned model adapts to Wikipedia-style text from WikiText-2.\")\n",
    "print(\"You may notice more formal, encyclopedic language patterns.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Checkpoint Recovery Demo\n",
    "\n",
    "Demonstrate loading from a saved checkpoint and verifying model state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checkpoint Recovery Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a fresh model\n",
    "fresh_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load checkpoint\n",
    "nvme_backend = NVMeBackend(base_path='/tmp/gpt2_finetune_checkpoints')\n",
    "latest_path = nvme_backend.get_latest_checkpoint()\n",
    "\n",
    "if latest_path:\n",
    "    checkpoint = nvme_backend.load(latest_path)\n",
    "    fresh_model.load_state_dict(checkpoint.model_state_dict)\n",
    "    \n",
    "    print(f\"Loaded checkpoint from step {checkpoint.step}\")\n",
    "    print(f\"Checkpoint metrics: {checkpoint.metrics}\")\n",
    "    \n",
    "    # Verify model works\n",
    "    fresh_model = fresh_model.to(device)\n",
    "    \n",
    "    val_loss, val_ppl = evaluate(fresh_model, val_loader, device)\n",
    "    print(f\"Validation loss after loading: {val_loss:.4f}\")\n",
    "    print(f\"Validation perplexity: {val_ppl:.1f}\")\n",
    "    \n",
    "    # Generate text to verify\n",
    "    print(\"\\nGeneration test after checkpoint load:\")\n",
    "    samples = generate_samples(fresh_model, tokenizer, [\"The history of\"], device, max_new_tokens=30)\n",
    "    print(f\"Generated: {samples[0][1]}\")\n",
    "    \n",
    "    print(\"\\nCheckpoint recovery successful!\")\n",
    "else:\n",
    "    print(\"No checkpoint found - run training first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Topology Change Simulation\n",
    "\n",
    "Demonstrate how elastic_harness handles dynamic world size changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Simulating Topology Changes\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Setup with elastic scaling managers (using a tiny model for demo)\n",
    "dummy_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "dummy_optimizer = torch.optim.AdamW(dummy_model.parameters(), lr=5e-5)\n",
    "\n",
    "# ElasticScalingManager combines LR and accumulation\n",
    "elastic_manager = ElasticScalingManager(\n",
    "    optimizer=dummy_optimizer,\n",
    "    scaling_config=ScalingConfig(\n",
    "        base_lr=5e-5,\n",
    "        base_batch_size=8,\n",
    "        base_world_size=4,\n",
    "        scaling_rule=ScalingRule.SQRT,\n",
    "        warmup_steps=100,\n",
    "    ),\n",
    "    accum_config=GradAccumulationConfig(\n",
    "        target_global_batch_size=512,\n",
    "        local_batch_size=8,\n",
    "        base_world_size=4,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"Initial state (4 GPUs):\")\n",
    "print(f\"  LR: {elastic_manager.current_lr:.2e}\")\n",
    "print(f\"  Accumulation steps: {elastic_manager.accumulation_steps}\")\n",
    "print(f\"  Global batch: {8 * 4 * elastic_manager.accumulation_steps}\")\n",
    "print()\n",
    "\n",
    "# Simulate losing 2 GPUs\n",
    "print(\"Worker failure: 4 GPUs -> 2 GPUs\")\n",
    "elastic_manager.on_topology_change(2)\n",
    "print(f\"  New LR: {elastic_manager.current_lr:.2e}\")\n",
    "print(f\"  New accumulation: {elastic_manager.accumulation_steps}\")\n",
    "print(f\"  Global batch: {8 * 2 * elastic_manager.accumulation_steps}\")\n",
    "print()\n",
    "\n",
    "# Simulate scaling up to 8 GPUs\n",
    "print(\"Scaling up: 2 GPUs -> 8 GPUs\")\n",
    "elastic_manager.on_topology_change(8)\n",
    "print(f\"  New LR: {elastic_manager.current_lr:.2e}\")\n",
    "print(f\"  New accumulation: {elastic_manager.accumulation_steps}\")\n",
    "print(f\"  Global batch: {8 * 8 * elastic_manager.accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a complete GPT-2 fine-tuning pipeline with `elastic_harness`:\n",
    "\n",
    "| Feature | Details |\n",
    "|---------|--------|\n",
    "| Model | GPT-2 (124M parameters, pretrained) |\n",
    "| Data | WikiText-2 with GPT-2 tokenization |\n",
    "| Training | 1000 steps, batch 8 (via accumulation) |\n",
    "| Learning Rate | 5e-5 with 100-step warmup |\n",
    "| Checkpointing | Memory + NVMe multi-tier |\n",
    "| Scaling | SQRT rule for topology changes |\n",
    "| Output | Coherent, Wikipedia-style text |\n",
    "\n",
    "**Key differences from the basic demo notebook:**\n",
    "- Uses **pretrained model** instead of training from scratch\n",
    "- Produces **coherent text** instead of gibberish\n",
    "- Lower learning rate (5e-5) appropriate for fine-tuning\n",
    "- Longer warmup (100 steps) for stable fine-tuning\n",
    "\n",
    "**elastic_harness components demonstrated:**\n",
    "- `MemorySnapshotBackend` - Fast in-memory checkpoints\n",
    "- `NVMeBackend` - Persistent disk checkpoints with cleanup\n",
    "- `LRScalingManager` - Learning rate warmup and topology scaling\n",
    "- `ElasticScalingManager` - Unified LR + accumulation management\n",
    "\n",
    "**For production deployment:**\n",
    "- Add `S3Backend` for durable cloud checkpoints\n",
    "- Use `CheckpointManager` for coordinated multi-tier saves\n",
    "- Deploy etcd for multi-node elastic rendezvous\n",
    "- Enable chaos testing to validate fault tolerance"
   ]
  }
 ]
}
